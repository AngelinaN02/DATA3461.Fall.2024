{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49cf8381",
   "metadata": {},
   "source": [
    "# Lab 6\n",
    "\n",
    "Scikit learn provides a large variety of algorithms for some common Machine Learning tasks, such as:\n",
    "\n",
    "* Classification\n",
    "* Regression\n",
    "* Clustering\n",
    "* Feature Selection\n",
    "* Anomaly Detection\n",
    "\n",
    "It also provides some datasets that you can use to test these algorithms:\n",
    "\n",
    "* Classification Datasets:\n",
    "    * Breast cancer wisconsin\n",
    "    * Iris plants (3-classes)\n",
    "    * Optical recognition of handwritten digits (10-classes)\n",
    "    * Wine (n-classes)\n",
    "\n",
    "* Regression Datasets: \n",
    "    * Boston house prices \n",
    "    * Diabetes\n",
    "    * Linnerrud (multiple regression)\n",
    "    * California Housing\n",
    "\n",
    "* Image: \n",
    "    * The Olivetti faces\n",
    "    * The Labeled Faces in the Wild face recognition\n",
    "    * Forest covertypes\n",
    "\n",
    "* NLP:\n",
    "    * News group\n",
    "    * Reuters Corpus Volume I \n",
    "\n",
    "* Other:\n",
    "    * Kddcup 99- Intrusion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e3b6e9",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Use the full [Kddcup](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) dataset to compare classification performance of 3 different classifiers. \n",
    "    * Separate the data into train, validation, and test. \n",
    "    * Use accuracy as the metric for assessing performance. \n",
    "    * For each classifier, identify the hyperparameters. Perform optimization over at least 2 hyperparameters.   \n",
    "    * Compare the performance of the optimal configuration of the classifiers.\n",
    "\n",
    "2. Pick the best algorithm in question 1. Create an ensemble of at least 25 models, and use them for the classification task. Identify the top and bottom 10% of the data in terms of uncertainty of the decision.\n",
    "\n",
    "3. Use 2 different feature selection algorithm to identify the 10 most important features for the task in question 1. Retrain classifiers in question 1 with just this subset of features and compare performance.\n",
    "\n",
    "4. Use the same data, removing the labels, and compare performance of 3 different clustering algorithms. Can you find clusters for each of the classes in question 1? \n",
    "\n",
    "5. Can you identify any clusters within the top/botton 10% identified in 2. What are their characteristics?\n",
    "\n",
    "6. Use the \"SA\" dataset to compare the performance of 3 different anomaly detection algorithms.\n",
    "\n",
    "7. Create a subsample of 250 datapoints, redo question 6, using Leave-one-out as the method of evaluation.\n",
    "\n",
    "8. Use the feature selection algorithm to identify the 5 most important features for the task in question 6, for each algorithm. Does the anomaly detection improve using less features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03a9391c-2999-434d-a5a7-ee09b700255e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    1     2   3    4      5   6   7   8   9   ...  32   33   34    35  \\\n",
      "0   0  tcp  http  SF  215  45076   0   0   0   0  ...   0  0.0  0.0  0.00   \n",
      "1   0  tcp  http  SF  162   4528   0   0   0   0  ...   1  1.0  0.0  1.00   \n",
      "2   0  tcp  http  SF  236   1228   0   0   0   0  ...   2  1.0  0.0  0.50   \n",
      "3   0  tcp  http  SF  233   2032   0   0   0   0  ...   3  1.0  0.0  0.33   \n",
      "4   0  tcp  http  SF  239    486   0   0   0   0  ...   4  1.0  0.0  0.25   \n",
      "\n",
      "    36   37   38   39   40       41  \n",
      "0  0.0  0.0  0.0  0.0  0.0  normal.  \n",
      "1  0.0  0.0  0.0  0.0  0.0  normal.  \n",
      "2  0.0  0.0  0.0  0.0  0.0  normal.  \n",
      "3  0.0  0.0  0.0  0.0  0.0  normal.  \n",
      "4  0.0  0.0  0.0  0.0  0.0  normal.  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_name=\"kddcup.data.corrected\"\n",
    "df=pd.read_csv(file_name, header=None)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef18662c",
   "metadata": {},
   "source": [
    "## Quick look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f1c631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_kddcup99\n",
    "D=fetch_kddcup99()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d561eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'feature_names', 'frame', 'target', 'target_names']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "875d2d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _kddcup99_dataset:\n",
      "\n",
      "Kddcup 99 dataset\n",
      "-----------------\n",
      "\n",
      "The KDD Cup '99 dataset was created by processing the tcpdump portions\n",
      "of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset,\n",
      "created by MIT Lincoln Lab [2]_. The artificial data (described on the `dataset's\n",
      "homepage <https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html>`_) was\n",
      "generated using a closed network and hand-injected attacks to produce a\n",
      "large number of different types of attack with normal activity in the\n",
      "background. As the initial goal was to produce a large training set for\n",
      "supervised learning algorithms, there is a large proportion (80.1%) of\n",
      "abnormal data which is unrealistic in real world, and inappropriate for\n",
      "unsupervised anomaly detection which aims at detecting 'abnormal' data, i.e.:\n",
      "\n",
      "* qualitatively different from normal data\n",
      "* in large minority among the observations.\n",
      "\n",
      "We thus transform the KDD Data set into two different data sets: SA and SF.\n",
      "\n",
      "* SA is obtained by simply selecting all the normal data, and a small\n",
      "  proportion of abnormal data to gives an anomaly proportion of 1%.\n",
      "\n",
      "* SF is obtained as in [3]_\n",
      "  by simply picking up the data whose attribute logged_in is positive, thus\n",
      "  focusing on the intrusion attack, which gives a proportion of 0.3% of\n",
      "  attack.\n",
      "\n",
      "* http and smtp are two subsets of SF corresponding with third feature\n",
      "  equal to 'http' (resp. to 'smtp').\n",
      "\n",
      "General KDD structure:\n",
      "\n",
      "================      ==========================================\n",
      "Samples total         4898431\n",
      "Dimensionality        41\n",
      "Features              discrete (int) or continuous (float)\n",
      "Targets               str, 'normal.' or name of the anomaly type\n",
      "================      ==========================================\n",
      "\n",
      "SA structure:\n",
      "\n",
      "================      ==========================================\n",
      "Samples total         976158\n",
      "Dimensionality        41\n",
      "Features              discrete (int) or continuous (float)\n",
      "Targets               str, 'normal.' or name of the anomaly type\n",
      "================      ==========================================\n",
      "\n",
      "SF structure:\n",
      "\n",
      "================      ==========================================\n",
      "Samples total         699691\n",
      "Dimensionality        4\n",
      "Features              discrete (int) or continuous (float)\n",
      "Targets               str, 'normal.' or name of the anomaly type\n",
      "================      ==========================================\n",
      "\n",
      "http structure:\n",
      "\n",
      "================      ==========================================\n",
      "Samples total         619052\n",
      "Dimensionality        3\n",
      "Features              discrete (int) or continuous (float)\n",
      "Targets               str, 'normal.' or name of the anomaly type\n",
      "================      ==========================================\n",
      "\n",
      "smtp structure:\n",
      "\n",
      "================      ==========================================\n",
      "Samples total         95373\n",
      "Dimensionality        3\n",
      "Features              discrete (int) or continuous (float)\n",
      "Targets               str, 'normal.' or name of the anomaly type\n",
      "================      ==========================================\n",
      "\n",
      ":func:`sklearn.datasets.fetch_kddcup99` will load the kddcup99 dataset; it\n",
      "returns a dictionary-like object with the feature matrix in the ``data`` member\n",
      "and the target values in ``target``. The \"as_frame\" optional argument converts\n",
      "``data`` into a pandas DataFrame and ``target`` into a pandas Series. The\n",
      "dataset will be downloaded from the web if necessary.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    .. [2] Analysis and Results of the 1999 DARPA Off-Line Intrusion\n",
      "           Detection Evaluation, Richard Lippmann, Joshua W. Haines,\n",
      "           David J. Fried, Jonathan Korba, Kumar Das.\n",
      "\n",
      "    .. [3] K. Yamanishi, J.-I. Takeuchi, G. Williams, and P. Milne. Online\n",
      "           unsupervised outlier detection using finite mixtures with\n",
      "           discounting learning algorithms. In Proceedings of the sixth\n",
      "           ACM SIGKDD international conference on Knowledge discovery\n",
      "           and data mining, pages 320-324. ACM Press, 2000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(D[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3c3c5b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'feature_names', 'frame', 'target', 'target_names']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cef559d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'back.', b'buffer_overflow.', b'ftp_write.', b'guess_passwd.',\n",
       "       b'imap.', b'ipsweep.', b'land.', b'loadmodule.', b'multihop.',\n",
       "       b'neptune.', b'nmap.', b'normal.', b'perl.', b'phf.', b'pod.',\n",
       "       b'portsweep.', b'rootkit.', b'satan.', b'smurf.', b'spy.',\n",
       "       b'teardrop.', b'warezclient.', b'warezmaster.'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(D[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ed0289b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(D[\"target\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aff034ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['duration',\n",
       " 'protocol_type',\n",
       " 'service',\n",
       " 'flag',\n",
       " 'src_bytes',\n",
       " 'dst_bytes',\n",
       " 'land',\n",
       " 'wrong_fragment',\n",
       " 'urgent',\n",
       " 'hot',\n",
       " 'num_failed_logins',\n",
       " 'logged_in',\n",
       " 'num_compromised',\n",
       " 'root_shell',\n",
       " 'su_attempted',\n",
       " 'num_root',\n",
       " 'num_file_creations',\n",
       " 'num_shells',\n",
       " 'num_access_files',\n",
       " 'num_outbound_cmds',\n",
       " 'is_host_login',\n",
       " 'is_guest_login',\n",
       " 'count',\n",
       " 'srv_count',\n",
       " 'serror_rate',\n",
       " 'srv_serror_rate',\n",
       " 'rerror_rate',\n",
       " 'srv_rerror_rate',\n",
       " 'same_srv_rate',\n",
       " 'diff_srv_rate',\n",
       " 'srv_diff_host_rate',\n",
       " 'dst_host_count',\n",
       " 'dst_host_srv_count',\n",
       " 'dst_host_same_srv_rate',\n",
       " 'dst_host_diff_srv_rate',\n",
       " 'dst_host_same_src_port_rate',\n",
       " 'dst_host_srv_diff_host_rate',\n",
       " 'dst_host_serror_rate',\n",
       " 'dst_host_srv_serror_rate',\n",
       " 'dst_host_rerror_rate',\n",
       " 'dst_host_srv_rerror_rate']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[\"feature_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06d46b3-6f10-4bd8-a232-b3b8292449ef",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172b36f9-221e-48a4-a877-69b3e3dd0ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (3428901, 41), y_train shape: (3428901,)\n",
      "X_val shape: (734765, 41), y_val shape: (734765,)\n",
      "X_test shape: (734765, 41), y_test shape: (734765,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Load Dataset\n",
    "feature_names = D[\"feature_names\"]  \n",
    "feature_names.append(\"label\")  # Add the target column name\n",
    "file_path = \"kddcup.data.corrected\"\n",
    "data = pd.read_csv(file_path, header=None, names=feature_names)\n",
    "\n",
    "# Step 2: Preprocess Data (Encoding)\n",
    "categorical_features = ['protocol_type', 'service', 'flag', 'land', 'logged_in', 'is_host_login', 'is_guest_login']\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    data[feature] = le.fit_transform(data[feature])\n",
    "\n",
    "# Encode the target column\n",
    "data['label'] = LabelEncoder().fit_transform(data['label'])\n",
    "\n",
    "# Split features and target\n",
    "X = data.drop(columns=['label'])\n",
    "y = data['label']\n",
    "\n",
    "# Step 3: Split Dataset into Train, Validation, and Test Sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Check shapes before scaling\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Step 4: Scale Features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform validation and test data\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Ensure shapes match after scaling\n",
    "print(f\"X_train_scaled shape: {X_train_scaled.shape}, y_train shape: {y_train.shape}\")\n",
    "\n",
    "# Step 5: Define Classifiers and Hyperparameter Grids\n",
    "classifiers = {\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'RandomForest': {'n_estimators': [50, 100], 'max_depth': [10, 20, None]},\n",
    "    'SVM': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
    "    'KNN': {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']}\n",
    "}\n",
    "\n",
    "# Step 6: Train and Optimize Classifiers\n",
    "best_models = {}\n",
    "val_accuracies = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"Optimizing {name}...\")\n",
    "    grid_search = GridSearchCV(clf, param_grids[name], cv=3, scoring='accuracy', verbose=1)\n",
    "    grid_search.fit(X_train_scaled, y_train)  # Use scaled training data\n",
    "    best_models[name] = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    y_pred_val = grid_search.best_estimator_.predict(X_val_scaled)\n",
    "    accuracy = accuracy_score(y_val, y_pred_val)\n",
    "    val_accuracies[name] = accuracy\n",
    "    print(f\"{name}: Best Params: {grid_search.best_params_}, Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Step 7: Test the Best Model\n",
    "best_model_name = max(val_accuracies, key=val_accuracies.get)\n",
    "best_model = best_models[best_model_name]\n",
    "\n",
    "y_pred_test = best_model.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ee3ec7-4716-4fff-80e9-bcd6411b0c78",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8866dc7c-890a-484a-99f8-19f42291bafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Use the Best Model from Exercise 1\n",
    "best_base_model = best_models[best_model_name]\n",
    "\n",
    "# Step 2: Create an Ensemble of 25 Models\n",
    "ensemble_model = BaggingClassifier(\n",
    "    base_estimator=best_base_model, \n",
    "    n_estimators=25, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the ensemble model on the training data\n",
    "ensemble_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 3: Evaluate the Ensemble Model on the Test Set\n",
    "y_pred_test_ensemble = ensemble_model.predict(X_test_scaled)\n",
    "ensemble_accuracy = accuracy_score(y_test, y_pred_test_ensemble)\n",
    "print(f\"Ensemble Test Accuracy: {ensemble_accuracy:.4f}\")\n",
    "\n",
    "# Step 4: Estimate Uncertainty for Each Data Point\n",
    "# Get the probabilities (if the base model supports it) or use votes\n",
    "if hasattr(ensemble_model, \"predict_proba\"):\n",
    "    probabilities = ensemble_model.predict_proba(X_test_scaled)\n",
    "    uncertainty = np.max(probabilities, axis=1)  # Confidence in the most likely class\n",
    "    uncertainty = 1 - uncertainty  # Higher values indicate more uncertainty\n",
    "else:\n",
    "    predictions = np.array([estimator.predict(X_test_scaled) for estimator in ensemble_model.estimators_]).T\n",
    "    uncertainty = np.mean(predictions.std(axis=1))  # Use variance in predictions as a proxy for uncertainty\n",
    "\n",
    "# Step 5: Identify Top and Bottom 10% Based on Uncertainty\n",
    "num_points = len(uncertainty)\n",
    "top_10_percent_indices = np.argsort(uncertainty)[-int(0.1 * num_points):]  # Top 10% most uncertain\n",
    "bottom_10_percent_indices = np.argsort(uncertainty)[:int(0.1 * num_points)]  # Bottom 10% least uncertain\n",
    "\n",
    "# Extract the corresponding data points\n",
    "top_10_percent = X_test.iloc[top_10_percent_indices]\n",
    "bottom_10_percent = X_test.iloc[bottom_10_percent_indices]\n",
    "\n",
    "# Output statistics\n",
    "print(f\"Top 10% Uncertainty: {uncertainty[top_10_percent_indices].mean():.4f}\")\n",
    "print(f\"Bottom 10% Uncertainty: {uncertainty[bottom_10_percent_indices].mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f584f-dd13-4e11-8eca-df52bbbf123e",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a8bd9-0825-4061-9fd6-0dd2a66f625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Use Feature Selection Algorithms\n",
    "# Select top 10 features using two different algorithms\n",
    "\n",
    "# Algorithm 1: ANOVA F-statistic\n",
    "selector_f_classif = SelectKBest(score_func=f_classif, k=10)\n",
    "X_train_f_classif = selector_f_classif.fit_transform(X_train_scaled, y_train)\n",
    "X_val_f_classif = selector_f_classif.transform(X_val_scaled)\n",
    "X_test_f_classif = selector_f_classif.transform(X_test_scaled)\n",
    "\n",
    "selected_features_f_classif = selector_f_classif.get_support(indices=True)\n",
    "print(\"Selected Features by ANOVA F-statistic:\", selected_features_f_classif)\n",
    "\n",
    "# Algorithm 2: Mutual Information\n",
    "selector_mutual_info = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "X_train_mutual_info = selector_mutual_info.fit_transform(X_train_scaled, y_train)\n",
    "X_val_mutual_info = selector_mutual_info.transform(X_val_scaled)\n",
    "X_test_mutual_info = selector_mutual_info.transform(X_test_scaled)\n",
    "\n",
    "selected_features_mutual_info = selector_mutual_info.get_support(indices=True)\n",
    "print(\"Selected Features by Mutual Information:\", selected_features_mutual_info)\n",
    "\n",
    "# Step 2: Retrain Classifiers with Selected Features\n",
    "retrain_results = {}\n",
    "\n",
    "# Retrain classifiers on ANOVA F-statistic selected features\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train_f_classif, y_train)\n",
    "    y_pred_val = clf.predict(X_val_f_classif)\n",
    "    accuracy = accuracy_score(y_val, y_pred_val)\n",
    "    retrain_results[f\"{name}_f_classif\"] = accuracy\n",
    "    print(f\"{name} (ANOVA F-statistic): Validation Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "# Retrain classifiers on Mutual Information selected features\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train_mutual_info, y_train)\n",
    "    y_pred_val = clf.predict(X_val_mutual_info)\n",
    "    accuracy = accuracy_score(y_val, y_pred_val)\n",
    "    retrain_results[f\"{name}_mutual_info\"] = accuracy\n",
    "    print(f\"{name} (Mutual Information): Validation Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "# Step 3: Test the Best Classifier on Selected Features\n",
    "best_retrain_model_name = max(retrain_results, key=retrain_results.get)\n",
    "best_retrain_model_type, feature_selection_method = best_retrain_model_name.split(\"_\")\n",
    "best_retrain_model = classifiers[best_retrain_model_type]\n",
    "\n",
    "if feature_selection_method == \"f_classif\":\n",
    "    X_test_selected = X_test_f_classif\n",
    "elif feature_selection_method == \"mutual_info\":\n",
    "    X_test_selected = X_test_mutual_info\n",
    "\n",
    "y_pred_test = best_retrain_model.predict(X_test_selected)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\\nBest Retrained Model: {best_retrain_model_name}\")\n",
    "print(f\"Test Accuracy with Selected Features: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c108c3b4-fcfd-4c7d-8183-acf8f467d841",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81ae521-ec22-4dbd-97c9-9393bb7804a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
    "\n",
    "# Step 1: Use the same dataset without labels\n",
    "X_train_cluster = X_train_scaled\n",
    "X_test_cluster = X_test_scaled\n",
    "\n",
    "# Step 2: Define Clustering Algorithms\n",
    "clustering_algorithms = {\n",
    "    \"KMeans\": KMeans(n_clusters=len(np.unique(y_train)), random_state=42),\n",
    "    \"DBSCAN\": DBSCAN(eps=0.5, min_samples=5),\n",
    "    \"Agglomerative\": AgglomerativeClustering(n_clusters=len(np.unique(y_train)))\n",
    "}\n",
    "\n",
    "# Step 3: Train and Evaluate Clustering Algorithms\n",
    "clustering_results = {}\n",
    "\n",
    "for name, model in clustering_algorithms.items():\n",
    "    print(f\"Running {name}...\")\n",
    "    # Train the clustering model\n",
    "    cluster_labels_train = model.fit_predict(X_train_cluster)\n",
    "    cluster_labels_test = model.fit_predict(X_test_cluster)\n",
    "    \n",
    "    # Evaluate clustering using Adjusted Rand Index (ARI)\n",
    "    ari_score = adjusted_rand_score(y_test, cluster_labels_test)\n",
    "    \n",
    "    # Evaluate clustering using Silhouette Score (only for non-negative cluster labels)\n",
    "    if len(set(cluster_labels_test)) > 1:\n",
    "        silhouette = silhouette_score(X_test_cluster, cluster_labels_test)\n",
    "    else:\n",
    "        silhouette = -1  # Silhouette score is undefined for single cluster\n",
    "    \n",
    "    clustering_results[name] = {\n",
    "        \"ARI\": ari_score,\n",
    "        \"Silhouette Score\": silhouette\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} - Adjusted Rand Index: {ari_score:.4f}, Silhouette Score: {silhouette:.4f}\")\n",
    "\n",
    "# Step 4: Map Clusters to Classes (for KMeans only as an example)\n",
    "kmeans = KMeans(n_clusters=len(np.unique(y_train)), random_state=42)\n",
    "kmeans.fit(X_train_cluster)\n",
    "cluster_labels_test = kmeans.predict(X_test_cluster)\n",
    "\n",
    "# Map each cluster to the majority class in that cluster\n",
    "from scipy.stats import mode\n",
    "cluster_to_class_map = {}\n",
    "for cluster in np.unique(cluster_labels_test):\n",
    "    indices = np.where(cluster_labels_test == cluster)\n",
    "    majority_class = mode(y_test[indices]).mode[0]\n",
    "    cluster_to_class_map[cluster] = majority_class\n",
    "\n",
    "print(\"\\nCluster-to-Class Mapping (KMeans):\")\n",
    "for cluster, cls in cluster_to_class_map.items():\n",
    "    print(f\"Cluster {cluster}: Class {cls}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef92b447-c0d6-4af5-ad2a-69ec9e583f46",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b714f55-efbf-44c1-b6ca-20bf260122da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Step 1: Use the Top and Bottom 10% Data Identified in Exercise 2\n",
    "X_top_10 = X_test_scaled[top_10_percent_indices]\n",
    "X_bottom_10 = X_test_scaled[bottom_10_percent_indices]\n",
    "\n",
    "print(f\"Top 10% Data Shape: {X_top_10.shape}, Bottom 10% Data Shape: {X_bottom_10.shape}\")\n",
    "\n",
    "# Step 2: Define Clustering Algorithms\n",
    "clustering_algorithms = {\n",
    "    \"KMeans\": KMeans(n_clusters=2, random_state=42),\n",
    "    \"DBSCAN\": DBSCAN(eps=0.5, min_samples=5),\n",
    "    \"Agglomerative\": AgglomerativeClustering(n_clusters=2)\n",
    "}\n",
    "\n",
    "# Step 3: Cluster Top 10% and Bottom 10%\n",
    "cluster_results = {}\n",
    "\n",
    "for subset_name, subset_data in [(\"Top 10%\", X_top_10), (\"Bottom 10%\", X_bottom_10)]:\n",
    "    cluster_results[subset_name] = {}\n",
    "    for name, model in clustering_algorithms.items():\n",
    "        print(f\"Clustering {subset_name} Data with {name}...\")\n",
    "        cluster_labels = model.fit_predict(subset_data)\n",
    "        \n",
    "        # Evaluate clustering with Silhouette Score\n",
    "        if len(set(cluster_labels)) > 1:  # Check for more than 1 cluster\n",
    "            silhouette = silhouette_score(subset_data, cluster_labels)\n",
    "        else:\n",
    "            silhouette = -1  # Undefined silhouette score for single cluster\n",
    "        \n",
    "        cluster_results[subset_name][name] = {\n",
    "            \"Cluster Labels\": cluster_labels,\n",
    "            \"Silhouette Score\": silhouette\n",
    "        }\n",
    "        print(f\"{name} - Silhouette Score: {silhouette:.4f}\")\n",
    "\n",
    "# Step 4: Analyze Clusters\n",
    "# Example: Analyze characteristics of clusters for KMeans on Top 10% data\n",
    "kmeans_top_labels = cluster_results[\"Top 10%\"][\"KMeans\"][\"Cluster Labels\"]\n",
    "kmeans_top_clusters = {label: X_top_10[kmeans_top_labels == label] for label in np.unique(kmeans_top_labels)}\n",
    "\n",
    "print(\"\\nCharacteristics of Clusters in Top 10% (KMeans):\")\n",
    "for label, cluster_data in kmeans_top_clusters.items():\n",
    "    print(f\"Cluster {label}: Mean Feature Values:\\n{np.mean(cluster_data, axis=0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee66412f-9b9f-47d1-8c4b-3ea80817f382",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18770e8a-5bdd-4456-a426-d84878ecd79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Step 1: Load the \"SA\" Dataset\n",
    "# Assuming `SA_dataset.csv` is the file containing the data\n",
    "file_path = \"SA_dataset.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Split into features and labels\n",
    "X = data.drop(columns=[\"label\"])  \n",
    "y = data[\"label\"]  # 0 = normal, 1 = anomaly\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Define Anomaly Detection Algorithms\n",
    "anomaly_detectors = {\n",
    "    \"IsolationForest\": IsolationForest(random_state=42),\n",
    "    \"OneClassSVM\": OneClassSVM(kernel='rbf', gamma=0.1, nu=0.1),\n",
    "    \"LocalOutlierFactor\": LocalOutlierFactor(n_neighbors=20, novelty=True)\n",
    "}\n",
    "\n",
    "# Step 3: Train and Evaluate Each Algorithm\n",
    "results = {}\n",
    "\n",
    "for name, model in anomaly_detectors.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_scaled)  # Fit the model\n",
    "    \n",
    "    # Predict anomalies\n",
    "    if name == \"LocalOutlierFactor\":\n",
    "        # For LOF, use predict_proba to get anomaly scores on training data\n",
    "        y_pred = model.predict(X_scaled)\n",
    "    else:\n",
    "        y_pred = model.predict(X_scaled)\n",
    "    \n",
    "    # Convert predictions to binary: -1 = anomaly, 1 = normal\n",
    "    y_pred = np.where(y_pred == -1, 1, 0)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    precision = precision_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    \n",
    "    results[name] = {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1\n",
    "    }\n",
    "    print(f\"{name} - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Step 4: Display Results\n",
    "print(\"\\nAnomaly Detection Results:\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}: Precision = {metrics['Precision']:.4f}, Recall = {metrics['Recall']:.4f}, F1 Score = {metrics['F1 Score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37da3ae7-491d-46fd-9d8c-9d4e72757e92",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ff8a1a-7d07-4669-abd7-3bc9508c8efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Create a Subsample of 250 Data Points\n",
    "data_sampled = data.sample(n=250, random_state=42)  # Sample 250 points from the dataset\n",
    "X_subsample = data_sampled.drop(columns=[\"label\"])  \n",
    "y_subsample = data_sampled[\"label\"]\n",
    "\n",
    "# Scale the subsample\n",
    "X_subsample_scaled = scaler.fit_transform(X_subsample)\n",
    "\n",
    "# Step 2: Define Anomaly Detection Algorithms\n",
    "anomaly_detectors = {\n",
    "    \"IsolationForest\": IsolationForest(random_state=42),\n",
    "    \"OneClassSVM\": OneClassSVM(kernel='rbf', gamma=0.1, nu=0.1),\n",
    "    \"LocalOutlierFactor\": LocalOutlierFactor(n_neighbors=20, novelty=True)\n",
    "}\n",
    "\n",
    "# Step 3: Leave-One-Out Cross-Validation\n",
    "loo = LeaveOneOut()\n",
    "results_loo = {}\n",
    "\n",
    "for name, model in anomaly_detectors.items():\n",
    "    print(f\"Running Leave-One-Out for {name}...\")\n",
    "    precision_list, recall_list, f1_list = [], [], []\n",
    "    \n",
    "    for train_index, test_index in loo.split(X_subsample_scaled):\n",
    "        X_train_loo, X_test_loo = X_subsample_scaled[train_index], X_subsample_scaled[test_index]\n",
    "        y_train_loo, y_test_loo = y_subsample.iloc[train_index], y_subsample.iloc[test_index]\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train_loo)\n",
    "        \n",
    "        # Predict\n",
    "        if name == \"LocalOutlierFactor\":\n",
    "            y_pred_loo = model.predict(X_test_loo)\n",
    "        else:\n",
    "            y_pred_loo = model.predict(X_test_loo)\n",
    "        \n",
    "        # Convert predictions to binary: -1 = anomaly, 1 = normal\n",
    "        y_pred_loo = np.where(y_pred_loo == -1, 1, 0)\n",
    "        \n",
    "        # Evaluate\n",
    "        precision = precision_score([y_test_loo], [y_pred_loo], zero_division=1)\n",
    "        recall = recall_score([y_test_loo], [y_pred_loo], zero_division=1)\n",
    "        f1 = f1_score([y_test_loo], [y_pred_loo], zero_division=1)\n",
    "        \n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "    \n",
    "    # Compute average metrics\n",
    "    results_loo[name] = {\n",
    "        \"Precision\": np.mean(precision_list),\n",
    "        \"Recall\": np.mean(recall_list),\n",
    "        \"F1 Score\": np.mean(f1_list)\n",
    "    }\n",
    "    print(f\"{name} - Precision: {np.mean(precision_list):.4f}, Recall: {np.mean(recall_list):.4f}, F1 Score: {np.mean(f1_list):.4f}\")\n",
    "\n",
    "# Step 4: Display Results\n",
    "print(\"\\nLeave-One-Out Results:\")\n",
    "for name, metrics in results_loo.items():\n",
    "    print(f\"{name}: Precision = {metrics['Precision']:.4f}, Recall = {metrics['Recall']:.4f}, F1 Score = {metrics['F1 Score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5743083b-d8b5-42aa-85f8-1c70f87590df",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c024518-6784-4191-883c-936c0ad592b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, f_classif\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Use Feature Selection to Identify the Top 5 Features\n",
    "# Use both ANOVA F-statistic and Mutual Information\n",
    "\n",
    "# ANOVA F-statistic\n",
    "selector_f_classif = SelectKBest(score_func=f_classif, k=5)\n",
    "X_train_f_classif = selector_f_classif.fit_transform(X_subsample_scaled, y_subsample)\n",
    "X_test_f_classif = selector_f_classif.transform(X_subsample_scaled)\n",
    "\n",
    "# Mutual Information\n",
    "selector_mutual_info = SelectKBest(score_func=mutual_info_classif, k=5)\n",
    "X_train_mutual_info = selector_mutual_info.fit_transform(X_subsample_scaled, y_subsample)\n",
    "X_test_mutual_info = selector_mutual_info.transform(X_subsample_scaled)\n",
    "\n",
    "print(\"Top Features by ANOVA F-statistic:\", selector_f_classif.get_support(indices=True))\n",
    "print(\"Top Features by Mutual Information:\", selector_mutual_info.get_support(indices=True))\n",
    "\n",
    "# Step 2: Retrain Anomaly Detection Algorithms with Selected Features\n",
    "results_feature_selection = {}\n",
    "\n",
    "for name, model in anomaly_detectors.items():\n",
    "    for method, X_train_selected, X_test_selected in [\n",
    "        (\"ANOVA\", X_train_f_classif, X_test_f_classif),\n",
    "        (\"Mutual Info\", X_train_mutual_info, X_test_mutual_info)\n",
    "    ]:\n",
    "        print(f\"Training {name} with {method} selected features...\")\n",
    "        model.fit(X_train_selected)\n",
    "        \n",
    "        # Predict\n",
    "        if name == \"LocalOutlierFactor\":\n",
    "            y_pred = model.predict(X_test_selected)\n",
    "        else:\n",
    "            y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        # Convert predictions to binary: -1 = anomaly, 1 = normal\n",
    "        y_pred = np.where(y_pred == -1, 1, 0)\n",
    "        \n",
    "        # Evaluate\n",
    "        precision = precision_score(y_subsample, y_pred, zero_division=1)\n",
    "        recall = recall_score(y_subsample, y_pred, zero_division=1)\n",
    "        f1 = f1_score(y_subsample, y_pred, zero_division=1)\n",
    "        \n",
    "        results_feature_selection[f\"{name}_{method}\"] = {\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1\n",
    "        }\n",
    "        print(f\"{name} ({method}): Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}\")\n",
    "\n",
    "# Step 3: Compare Results\n",
    "print(\"\\nFeature Selection Results:\")\n",
    "for name, metrics in results_feature_selection.items():\n",
    "    print(f\"{name}: Precision = {metrics['Precision']:.4f}, Recall = {metrics['Recall']:.4f}, F1 Score = {metrics['F1 Score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754f640b-d03e-4c31-b678-4fb38d58fc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
